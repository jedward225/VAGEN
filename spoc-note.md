# SPOC Environment Analysis

## Installation Commands
```bash
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
source ~/.bashrc
sudo apt-get update -y && sudo apt-get install -y vulkan-tools mesa-vulkan-drivers

pip install ai2thor
sudo apt-get update
sudo apt-get install -y vulkan-tools libvulkan1 vulkan-utils
sudo apt-get install -y xvfb mesa-utils
sudo apt install net-tools
sudo apt-get install -y libvulkan1
sudo apt install x11-apps
```

---

# ğŸ¤– Agent Prompt å®Œæ•´åˆ†ææŠ¥å‘Š

## æ¦‚è¿°
SPOCç¯å¢ƒç»™GPT-4o-mini agentæä¾›çš„æ˜¯ä¸€ä¸ªå¤šå±‚æ¬¡ã€ç»“æ„åŒ–çš„promptç³»ç»Ÿï¼ŒåŒ…å«ç³»ç»Ÿæç¤ºã€å†å²åé¦ˆã€å½“å‰è§‚å¯Ÿå’Œæ ¼å¼è¦æ±‚ã€‚

---

## ğŸ“‹ Prompt ç»“æ„ç»„æˆ

### 1. **System Prompt** (æ€»ä½“æŒ‡å¯¼)

#### 1.1 ä»»åŠ¡å®šä¹‰éƒ¨åˆ† (å·²ä¼˜åŒ–)
**å½“å‰ä½¿ç”¨çš„System Prompt:**
```
You are a Stretch robot in a household environment. Your task is to find and fetch specific objects.
STRATEGY: 
1) Clarify your tasks and goals, use the map properly to explore the room, and navigate to the appropriate location
2) Try to find the target, extend your arm and pickup the object

IMPORTANT: Keep responses concise. 
In <think> tags: observation, reasoning and prediction
In <answer> tags: action name(s) only

VALID ACTIONS: moveahead, moveback, rotateright, rotateleft, rotateright_small, rotateleft_small, pickup, dropoff, move_arm_up, move_arm_down, move_arm_out, move_arm_in, wrist_open, wrist_close, move_arm_up_small, move_arm_down_small, move_arm_out_small, move_arm_in_small
You cannot use any other actions not listed above.
```

**ä¼˜åŒ–è¦ç‚¹:**
- âœ… **å¼ºè°ƒåœ°å›¾ä½¿ç”¨**: "use the map properly to explore" - æé«˜ç©ºé—´æ„ŸçŸ¥
- âœ… **ç®€åŒ–ç­–ç•¥**: ä»3æ­¥å‡ä¸º2æ­¥ï¼Œæ›´ç›´æ¥é«˜æ•ˆ
- âœ… **æ˜ç¡®åŠ¨ä½œåˆ—è¡¨**: ç›´æ¥åˆ—å‡ºæ‰€æœ‰å¯ç”¨åŠ¨ä½œ
- âœ… **å‡å°‘å†—ä½™**: ç§»é™¤è¿‡äºè¯¦ç»†çš„æ€è€ƒæ ¼å¼è¦æ±‚

#### 1.2 åŠ¨ä½œçº¦æŸ
- **æœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨**(20ä¸ª): moveahead, moveback, rotateright, rotateleft, pickup, dropoff, move_arm_up, move_arm_down, move_arm_out, move_arm_in, wrist_open, wrist_close + å°å¹…åº¦è°ƒæ•´ç‰ˆæœ¬
- **ç¦ç”¨åŠ¨ä½œ**: moveleft, moveright, lookup, lookdown

#### 1.3 è¾“å…¥è¾“å‡ºæ ¼å¼ (`free_think` - å·²ä¼˜åŒ–)
**å½“å‰ä½¿ç”¨æ ¼å¼:**
```
<think>...</think>
<answer>...</answer>
```

**ä¸åŸç‰ˆå¯¹æ¯”:**
- âŒ **åŸç‰ˆ** `grounding_worldmodeling`: éœ€è¦è¯¦ç»†çš„ observation/reasoning/prediction ä¸‰æ®µå¼
- âœ… **ä¼˜åŒ–ç‰ˆ** `free_think`: ç®€åŒ–ä¸ºä¸€ä¸ª think æ ‡ç­¾ï¼Œæ›´çµæ´»é«˜æ•ˆ
- ğŸ¯ **æ•ˆæœ**: å‡å°‘tokenæ¶ˆè€—ï¼Œæé«˜å“åº”é€Ÿåº¦ï¼Œä¿æŒæ ¸å¿ƒæ€è€ƒèƒ½åŠ›

#### 1.4 ç¯å¢ƒç†è§£æŒ‡å¯¼
- **æœºæ¢°è‡‚çŠ¶æ€**: z(ä¼¸å±•)/y(é«˜åº¦)/wrist(è…•éƒ¨è§’åº¦)/gripper(å¤¹çˆªçŠ¶æ€)
- **ä¸‰è§†å›¾åè°ƒ**: å¯¼èˆªç›¸æœºã€æ“ä½œç›¸æœºã€ä¿¯è§†åœ°å›¾
- **8æ­¥æ“ä½œæµç¨‹**: æ‰«æâ†’è§„åˆ’â†’å¯¼èˆªâ†’å®šä½â†’ä¼¸å±•â†’è°ƒæ•´â†’æŠ“å–â†’é‡Šæ”¾

#### 1.5 å¥–åŠ±æœºåˆ¶
- æ ¼å¼æ­£ç¡®: +0.5
- æˆåŠŸæ“ä½œ: +10.0

### 2. **Per-Turn Prompt** (æ¯è½®åŠ¨æ€ä¿¡æ¯)

#### 2.1 å†å²åŠ¨ä½œåé¦ˆæ¨¡æ¿ (action_template)
```
After your answer, the extracted valid action is [ä¸Šæ¬¡æå–çš„åŠ¨ä½œ].
The environment feedback is: [æˆåŠŸ/å¤±è´¥ä¿¡æ¯]
reward: [å¥–åŠ±å€¼]
done: [ä»»åŠ¡å®ŒæˆçŠ¶æ€]
After that, the observation is:
[å½“å‰è§‚å¯Ÿ...]
Arm State: [æœºæ¢°è‡‚çŠ¶æ€]
Human Instruction: [ä»»åŠ¡æŒ‡ä»¤]
Decide your next action(s).
```

#### 2.2 ç¯å¢ƒåé¦ˆç±»å‹
- **æˆåŠŸ**: `"Action 'moveahead' executed successfully."`
- **å¤±è´¥**: `"Action 'pickup' FAILED - No object in range. Try a different approach."`
- **æ— æ•ˆ**: `"No valid action was provided. Please specify a valid action."`

#### 2.3 å½“å‰è§‚å¯Ÿä¿¡æ¯
- **è§†è§‰æè¿°**: ä¸‰ä¸ªæ‘„åƒå¤´çš„æ–‡å­—æè¿°
- **å›¾åƒæ•°æ®**: 3ä¸ªå›¾åƒå ä½ç¬¦ `<image> <image> <image>`
  - å¯¼èˆªç›¸æœºå›¾åƒ (384x224)
  - æ“ä½œç›¸æœºå›¾åƒ (384x224) 
  - ä¿¯è§†åœ°å›¾å›¾åƒ (396x224ï¼Œå¸¦è½¨è¿¹)
- **æœºæ¢°è‡‚çŠ¶æ€**: `z=0.1m, y=0.8m, wrist=15Â°, gripper=holding_mug`
- **ä»»åŠ¡æŒ‡ä»¤**: `"Find and fetch the coffee mug"`

### 3. **Format Prompt** (è¡Œä¸ºçº¦æŸ - å·²ä¼˜åŒ–)
**å½“å‰è®¾ç½®:**
```
You can take up to 3 action(s) at a time, separated by ','.
IMPORTANT: Limit responses to 1-3 actions maximum. Avoid repeating actions.
You should first give your thought process with your observation, reasoning, and prediction of next state, then your answer.
Include both camera views and arm proprioception in observation and prediction.

Your response should be in the format of:
<think><observation>...</observation><reasoning>...</reasoning><prediction>...</prediction></think><answer>...</answer>
```

---

## ğŸ“Š å®Œæ•´çš„Multi-Turnå¯¹è¯ç¤ºä¾‹

### Round 1 (åˆå§‹)
**User Prompt:**
```
[Initial Observation]:
Visual Observation: [Navigation camera showing kitchen, Manipulation camera showing counter edge, Top-down map showing robot position] <image> <image> <image>
Arm State: z=0.0m, y=0.8m, wrist=0Â°, gripper=empty
Human Instruction: Find and fetch the coffee mug
Decide your next action(s).
```

**Agent Response:**
```
<think>
<observation>Navigation view: I am in a kitchen environment with a counter directly ahead. The target coffee mug is visible on the counter surface. Manipulation view: I can see the counter edge but the mug is not yet within reach. Arm state: Retracted (z=0.0m, y=0.8m), wrist neutral, gripper empty.</observation>
<reasoning>I need to navigate closer to the counter to bring the mug within manipulation range.</reasoning>
<prediction>Navigation view: I will be closer to the counter with the mug accessible. Manipulation view: The mug will be visible and within arm's reach. Arm state: Still retracted but in optimal position.</prediction>
</think>
<answer>moveahead, moveahead</answer>
```

### Round 2 (å¸¦å†å²åé¦ˆ)
**User Prompt:**
```
After your answer, the extracted valid action is moveahead, moveahead.
The environment feedback is: Action 'moveahead' executed successfully.
reward: 0.5
done: False
After that, the observation is:
Visual Observation: [Navigation camera showing closer view of mug, Manipulation camera showing mug in view, Top-down map showing updated position] <image> <image> <image>
Arm State: z=0.0m, y=0.8m, wrist=0Â°, gripper=empty
Human Instruction: Find and fetch the coffee mug
Decide your next action(s).
```

---

## ğŸ” å…³é”®ç‰¹æ€§åˆ†æ

### 1. **å†å²æ„ŸçŸ¥èƒ½åŠ›**
- Agentèƒ½è·å¾—**ä¸Šä¸€è½®åŠ¨ä½œçš„æ‰§è¡Œç»“æœ**
- åŒ…å«**æˆåŠŸ/å¤±è´¥çš„å…·ä½“åŸå› **
- æä¾›**å¥–åŠ±åé¦ˆ**å¼•å¯¼å­¦ä¹ 

### 2. **å¤šæ¨¡æ€è¾“å…¥**
- **æ–‡å­—æè¿°** + **3ä¸ªå®æ—¶å›¾åƒ**
- **æœºæ¢°è‡‚æœ¬ä½“æ„ŸçŸ¥**æ•°æ®
- **ç©ºé—´åœ°å›¾ä¿¡æ¯**ï¼ˆå¸¦è½¨è¿¹æ˜¾ç¤ºï¼‰

### 3. **æ ¼å¼å¼ºçº¦æŸ**
- ä¸¥æ ¼è¦æ±‚ç‰¹å®šçš„XMLæ ‡ç­¾æ ¼å¼
- é™åˆ¶åŠ¨ä½œæ•°é‡ï¼ˆ1-3ä¸ªï¼‰
- ç¦æ­¢ä½¿ç”¨æœªåˆ—å‡ºçš„åŠ¨ä½œ

### 4. **é€æ­¥å¼•å¯¼ç­–ç•¥**
- è§‚å¯Ÿ â†’ æ¨ç† â†’ é¢„æµ‹ â†’ è¡ŒåŠ¨
- é¼“åŠ±**å¤šæ­¥è§„åˆ’**å’Œ**çŠ¶æ€é¢„æµ‹**
- å¼ºè°ƒ**è§†è§‰-è¿åŠ¨åè°ƒ**

### 5. **é”™è¯¯çº æ­£æœºåˆ¶**
- å¤±è´¥åŠ¨ä½œä¼šæä¾›**å…·ä½“å¤±è´¥åŸå› **
- å»ºè®®"å°è¯•ä¸åŒæ–¹æ³•"
- é€šè¿‡å¥–åŠ±ä¿¡å·å¼ºåŒ–æ­£ç¡®è¡Œä¸º

---

## ğŸ“ˆ Promptè®¾è®¡ä¼˜åŠ¿

1. **ç»“æ„åŒ–æ€ç»´**: å¼ºåˆ¶agentè¿›è¡Œè§‚å¯Ÿ-æ¨ç†-é¢„æµ‹çš„å®Œæ•´æ€è€ƒæµç¨‹
2. **å¤šæ¨¡æ€èåˆ**: æœ‰æ•ˆæ•´åˆè§†è§‰ã€æœ¬ä½“æ„ŸçŸ¥å’Œåœ°å›¾ä¿¡æ¯
3. **åœ¨çº¿å­¦ä¹ **: é€šè¿‡å³æ—¶åé¦ˆä¸æ–­è°ƒæ•´ç­–ç•¥
4. **é”™è¯¯å®¹å¿**: æä¾›å¤±è´¥ä¿¡æ¯è€Œéç®€å•æƒ©ç½š
5. **ä»»åŠ¡å¯¼å‘**: æ˜ç¡®çš„åˆ†æ­¥ç­–ç•¥æŒ‡å¯¼

è¿™ä¸ªpromptç³»ç»Ÿä¸ºrobot learningæä¾›äº†ä¸€ä¸ªéå¸¸å…¨é¢å’Œç»“æ„åŒ–çš„äº¤äº’æ¡†æ¶ã€‚

---

## ğŸ§ª æµ‹è¯•é…ç½®ä¼˜åŒ–

### GPT-4oæµ‹è¯•é…ç½® (`gpt4o_spoc.py`)

#### **ç¯å¢ƒé…ç½®**
```python
env_config = SpocEnvConfig(
    prompt_format="free_think",      # ä½¿ç”¨ç®€åŒ–æ ¼å¼
    max_actions_per_step=3,          # ä¿æŒçµæ´»æ€§
    action_sep=",",                  # é€—å·åˆ†éš”åŠ¨ä½œ
    image_placeholder="<image>"      # ä¸‰å›¾åƒè¾“å…¥
)
```

#### **æ¨¡å‹é…ç½®**  
```python
MODEL_CONFIG = {
    "model_name": "gpt-4o-mini",
    "max_tokens": 1024,              # ğŸ¯ å·²ä¼˜åŒ–: å¢åŠ è¾“å‡ºç©ºé—´
    "temperature": 0.2,              # ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§
    "seed": 42                       # å›ºå®šç§å­ä¾¿äºå¤ç°
}
```

#### **æµ‹è¯•è®¾ç½®**
```python
TEST_CONFIG = {
    "task_types": ["ObjectNavType", "RoomNav", "RoomVisit"],  # ğŸ¯ å·²ä¼˜åŒ–: å¤šä»»åŠ¡ç±»å‹
    "episodes_per_task": 2,          # ğŸ¯ å·²ä¼˜åŒ–: æ¯ç±»2ä¸ªepisode
    "max_steps": 60,                 # å……åˆ†çš„æ¢ç´¢æ—¶é—´
    "save_images": True,             # ä¿å­˜è¯¦ç»†å¯è§†åŒ–
    "image_interval": 5,             # ğŸ¯ å·²ä¼˜åŒ–: æ¯5æ­¥ä¿å­˜ä¸€æ¬¡
    "debug_mode": True               # è¯¦ç»†æ—¥å¿—è¾“å‡º
}
```

---

## ğŸ—ºï¸ åœ°å›¾ç”Ÿæˆé—®é¢˜è§£å†³æ–¹æ¡ˆ

### é—®é¢˜èƒŒæ™¯
- åŸå§‹ä»£ç ä½¿ç”¨å¤æ‚çš„`SPOCMapVisualizer`å­˜åœ¨å…¼å®¹æ€§é—®é¢˜
- `VisualizeWaypoints`åœ¨å½“å‰AI2-THORç‰ˆæœ¬ä¸­ä¸æ”¯æŒ
- éœ€è¦å¯é çš„top-downåœ°å›¾ç”Ÿæˆå¸¦è½¨è¿¹æ˜¾ç¤º

### è§£å†³æ–¹æ¡ˆ
ä½¿ç”¨ç¯å¢ƒå†…ç½®çš„`env.get_top_down_map(include_path=True, path_width=0.15)`æ–¹æ³•ï¼š

```python
# æ­£ç¡®çš„åœ°å›¾ç”Ÿæˆæ–¹æ³•
enhanced_map = self.env.get_top_down_map(include_path=True, path_width=0.15)
map_img = Image.fromarray(enhanced_map).convert('RGB')
```

### å…³é”®ä¿®æ”¹
1. åˆ é™¤`gpt4o_mini_spoc_vis.py`ä¾èµ–
2. ç®€åŒ–`_create_combined_image()`æ–¹æ³•
3. ä½¿ç”¨ç»è¿‡éªŒè¯çš„åœ°å›¾ç”ŸæˆAPI
4. æ·»åŠ JSONåºåˆ—åŒ–ä¿®å¤ç”¨äºNumPyç±»å‹

### ç»“æœ
- âœ… ç”Ÿæˆå¸¦æ©™è‰²è½¨è¿¹çº¿çš„ä¿¯è§†åœ°å›¾
- âœ… è·¯å¾„å®½åº¦0.15ï¼Œæ¸…æ™°å¯è§
- âœ… å…¼å®¹å½“å‰AI2-THORç‰ˆæœ¬
- âœ… æ— éœ€å¤æ‚çš„åæ ‡è½¬æ¢
