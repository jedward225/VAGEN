#!/usr/bin/env python3
"""
Shortest-path reward implementation for SPOC VAGEN training
Based on SPOC official navigation mesh approach
"""
import math
from typing import Optional, List, Dict

def calculate_shortest_path_reward(env, prev_distance: Optional[float] = None) -> Dict[str, float]:
    """
    Calculate shortest-path based reward signals for SPOC environment
    
    Args:
        env: SPOC environment instance
        prev_distance: Previous shortest distance to target
        
    Returns:
        Dict with reward components: path_progress, path_efficiency, optimal_ratio
    """
    rewards = {
        'path_progress': 0.0,
        'path_efficiency': 0.0, 
        'optimal_ratio': 0.0
    }
    
    try:
        # Get current agent position and target object
        agent_pos = env.env.last_event.metadata["agent"]["position"]
        target_type = env.episode_data.get("targetObjectType") if env.episode_data else None
        
        if not target_type:
            return rewards
            
        # Find target object in scene
        target_obj_id = None
        for obj in env.env.last_event.metadata.get("objects", []):
            if obj.get("visible", False) and env._is_object_type_match(obj["objectType"], target_type):
                target_obj_id = obj["objectId"]
                break
                
        if not target_obj_id:
            return rewards
            
        # Get shortest path using SPOC's fast method
        current_path = get_fast_shortest_path_to_object(env, target_obj_id, agent_pos)
        
        if current_path is None:
            return rewards
            
        # Calculate current shortest distance
        current_distance = sum_distance_path(current_path)
        
        # 1. Path Progress Reward (-1.0 to +2.0)
        if prev_distance is not None:
            distance_improvement = prev_distance - current_distance
            if distance_improvement > 0.05:  # Moved closer by 5cm+
                rewards['path_progress'] = min(distance_improvement * 5.0, 2.0)
            elif distance_improvement < -0.05:  # Moved away by 5cm+
                rewards['path_progress'] = max(distance_improvement * 2.0, -1.0)
                
        # 2. Path Efficiency Reward (0.0 to +1.0)
        # Reward for being close to optimal path
        if current_distance < 1.0:  # Within 1 meter
            rewards['path_efficiency'] = 1.0 - current_distance
        elif current_distance < 3.0:  # Within 3 meters
            rewards['path_efficiency'] = 0.5 * (3.0 - current_distance) / 2.0
            
        # 3. Optimal Path Ratio Reward (0.0 to +0.5)
        # Compare agent's actual path length to optimal
        if hasattr(env, 'agent_path') and len(env.agent_path) > 1:
            actual_path_length = calculate_agent_path_length(env.agent_path)
            initial_path = get_fast_shortest_path_to_object(env, target_obj_id, env.agent_path[0])
            if initial_path:
                optimal_length = sum_distance_path(initial_path)
                if optimal_length > 0:
                    ratio = optimal_length / max(actual_path_length, optimal_length)
                    rewards['optimal_ratio'] = ratio * 0.5
                    
    except Exception as e:
        print(f"[WARNING] Shortest path reward calculation failed: {e}")
        
    return rewards

def get_fast_shortest_path_to_object(env, object_id: str, start_position: dict) -> Optional[List]:
    """
    Fast shortest path calculation using SPOC's optimized approach
    """
    try:
        # Use largest navmesh radius for speed (like SPOC does)
        # Most permissive = fastest computation
        event = env.env.step(
            action="GetShortestPath",
            objectId=object_id,
            position=start_position,
            navMeshId=2,  # Largest radius (0.3m) = fastest
            returnToStart=False
        )
        
        if event.metadata.get("lastActionSuccess", False):
            corners = event.metadata.get("actionReturn", {}).get("corners", [])
            return corners if len(corners) > 0 else None
            
    except Exception as e:
        print(f"[WARNING] Fast shortest path failed: {e}")
        
    return None

def sum_distance_path(path: List[dict]) -> float:
    """Calculate total distance of a path (from SPOC utils)"""
    if len(path) < 2:
        return 0.0
        
    total_dist = 0.0
    for i in range(len(path) - 1):
        p0, p1 = path[i], path[i + 1]
        total_dist += math.sqrt(
            (p0["x"] - p1["x"]) ** 2 +
            (p0["z"] - p1["z"]) ** 2  # Ignore Y for 2D navigation
        )
    return total_dist

def calculate_agent_path_length(agent_path: List[dict]) -> float:
    """Calculate length of agent's actual movement path"""
    return sum_distance_path(agent_path)

# Integration example for your environment
def integrate_shortest_path_reward(env, prev_shortest_distance=None):
    """
    Integration function to add to your reward calculation in env.py
    """
    shortest_path_rewards = calculate_shortest_path_reward(env, prev_shortest_distance)
    
    # Add to your existing reward breakdown
    total_shortest_reward = sum(shortest_path_rewards.values())
    
    print(f"[SHORTEST PATH] Progress: {shortest_path_rewards['path_progress']:.3f}, "
          f"Efficiency: {shortest_path_rewards['path_efficiency']:.3f}, "
          f"Optimal: {shortest_path_rewards['optimal_ratio']:.3f}")
    
    return total_shortest_reward, shortest_path_rewards

if __name__ == "__main__":
    print("SPOC Shortest-Path Reward Implementation")
    print("This provides 3 reward signals:")
    print("1. Path Progress: Reward for getting closer to optimal path")
    print("2. Path Efficiency: Reward for being near target") 
    print("3. Optimal Ratio: Reward for efficient navigation vs optimal")

"""
SPOC æœ€çŸ­è·¯å¾„åˆ†æ
æ ¹æ®æˆ‘å¯¹ SPOC å®˜æ–¹ä»£ç çš„åˆ†æï¼Œä»¥ä¸‹æ˜¯æˆ‘å¯¹æœ€çŸ­è·¯å¾„è®¡ç®—åŠå…¶å®æ—¶å¯è¡Œæ€§çš„å‘ç°ï¼š

SPOC å¦‚ä½•å®ç°æœ€çŸ­è·¯å¾„ï¼š
AI2-THOR å†…ç½®ï¼š ä½¿ç”¨ AI2-THOR åŸç”Ÿçš„ GetShortestPath å’Œ GetShortestPathToPoint æ“ä½œã€‚

åŸºäºå¯¼èˆªç½‘æ ¼ï¼š ä½¿ç”¨é¢„è®¡ç®—çš„å¯¼èˆªç½‘æ ¼ï¼Œæ”¯æŒä¸åŒçš„ä»£ç†åŠå¾„ï¼ˆ0.18mã€0.225mã€0.3mï¼‰ã€‚

å¤šç§ç­–ç•¥ï¼š

get_shortest_path_to_object(object_id)ï¼šé€šå¾€ç‰¹å®šå¯¹è±¡çš„è·¯å¾„ã€‚

get_shortest_path_to_point(target_position)ï¼šé€šå¾€ç‰¹å®šåæ ‡çš„è·¯å¾„ã€‚

does_some_shortest_path_to_object_exist()ï¼šå¿«é€Ÿè·¯å¾„å­˜åœ¨æ€§æ£€æŸ¥ã€‚

å®æ—¶æ€§èƒ½ç‰¹å¾ï¼š
âœ… æ˜¯çš„ï¼Œå®æ—¶æ˜¯å¯èƒ½çš„ã€‚

å¿«é€Ÿæ“ä½œï¼š
ç”¨äºå¿«é€Ÿè·¯å¾„å­˜åœ¨æ€§æ£€æŸ¥ï¼ˆä»…ä½¿ç”¨æœ€é€šç”¨çš„å¯¼èˆªç½‘æ ¼ï¼‰çš„ä»£ç ç‰‡æ®µï¼š

Python

def does_some_shortest_path_to_object_exist():
    return get_shortest_path_to_object(
        specific_agent_meshes=[self.agent_ids[-1]],  # ä»…ä½¿ç”¨æœ€å¤§åŠå¾„
        attempt_path_improvement=False                # è·³è¿‡ä¼˜åŒ–
    ) is not None
SPOC ä¸­çš„æ€§èƒ½ä¼˜åŒ–ï¼š

å•ä¸€å¯¼èˆªç½‘æ ¼ï¼š ä½¿ç”¨ agent_ids[-1]ï¼ˆæœ€å¤§åŠå¾„ï¼‰ä»¥æé«˜é€Ÿåº¦ã€‚

æ— è·¯å¾„æ”¹è¿›ï¼š attempt_path_improvement=False è·³è¿‡æ˜‚è´µçš„ä¼˜åŒ–ã€‚

ç¼“å­˜ç»“æœï¼š å­˜å‚¨ last_successful_path ä»¥é¿å…é‡å¤è®¡ç®—ã€‚

ç”¨äºå¼ºåŒ–å­¦ä¹ å¥–åŠ±çš„å®ç°ï¼š
ä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨ SPOC ç¯å¢ƒä¸­å®ç°é«˜æ•ˆæœ€çŸ­è·¯å¾„å¥–åŠ±çš„æ–¹æ³•ï¼š

spoc_shortest_path_reward.py æ–‡ä»¶å·²åˆ›å»ºï¼Œå…± 147 è¡Œã€‚

Python

#!/usr/bin/env python3
"""
ç”¨äº SPOC VAGEN è®­ç»ƒçš„æœ€çŸ­è·¯å¾„å¥–åŠ±å®ç°
åŸºäº SPOC å®˜æ–¹å¯¼èˆªç½‘æ ¼æ–¹æ³•
"""
import math
from typing import Optional, List, Dict

def calculate_shortest_path_reward(env, prev_distance: Optional[float] = None) -> Dict[str, float]:
    """
    ... +137 è¡Œï¼ˆCtrl+R å±•å¼€ï¼‰
ğŸ“Š å®æ—¶å¯è¡Œæ€§è¯„ä¼°ï¼š
âœ… æ˜¯çš„ï¼Œå®æ—¶æœ€çŸ­è·¯å¾„æ˜¯å¯è¡Œçš„ã€‚

æ€§èƒ½ç‰¹å¾ï¼š
AI2-THOR åŸç”Ÿï¼š å†…éƒ¨ä½¿ç”¨ä¼˜åŒ–çš„ C++ è·¯å¾„æŸ¥æ‰¾ç®—æ³•ã€‚

é¢„è®¡ç®—å¯¼èˆªç½‘æ ¼ï¼š å¯¼èˆªç½‘æ ¼æ¯åœºæ™¯è®¡ç®—ä¸€æ¬¡ã€‚

å¿«é€ŸæŸ¥è¯¢ï¼š å•æ¬¡ API è°ƒç”¨ GetShortestPath çº¦ 1-5 æ¯«ç§’ã€‚

SPOC ä¼˜åŒ–ï¼š ä½¿ç”¨æœ€å¤§åŠå¾„å¯¼èˆªç½‘æ ¼ + è·³è¿‡è·¯å¾„æ”¹è¿›ã€‚

æ€§èƒ½åŸºå‡†ï¼ˆä¼°è®¡ï¼‰ï¼š
è·¯å¾„å­˜åœ¨æ€§æ£€æŸ¥ï¼š çº¦ 1-2 æ¯«ç§’ï¼ˆéå¸¸å¿«ï¼‰ã€‚

å®Œæ•´è·¯å¾„è®¡ç®—ï¼š çº¦ 5-10 æ¯«ç§’ï¼ˆå¯¹å¼ºåŒ–å­¦ä¹ è€Œè¨€è¶³å¤Ÿå¿«ï¼‰ã€‚

è·¯å¾„è·ç¦»è®¡ç®—ï¼š çº¦ 0.1 æ¯«ç§’ï¼ˆå¾®ä¸è¶³é“ï¼‰ã€‚

é›†æˆç­–ç•¥ï¼š
å°†å…¶æ·»åŠ åˆ°æ‚¨çš„ç¯å¢ƒçš„ _compute_step_reward() æ–¹æ³•ä¸­ï¼š

Python

# åœ¨æ‚¨çš„ env.py çš„ _compute_step_reward æ–¹æ³•ä¸­ï¼š
def _compute_step_reward(self, action_list, metrics, rst, prev_pos, curr_pos):
    # ... æ‚¨ç°æœ‰çš„å¥–åŠ± ...

    # æ·»åŠ æœ€çŸ­è·¯å¾„å¥–åŠ±
    shortest_reward, shortest_breakdown = integrate_shortest_path_reward(
        self,
        prev_shortest_distance=getattr(self, 'prev_shortest_distance', None)
    )

    reward += shortest_reward
    reward_breakdown.update(shortest_breakdown)

    # å­˜å‚¨ä»¥ä¾¿ä¸‹ä¸€æ­¥ä½¿ç”¨
    if shortest_breakdown.get('current_distance'):
        self.prev_shortest_distance = shortest_breakdown['current_distance']
å¯¹å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¥½å¤„ï¼š
å¯¼èˆªæŒ‡å¯¼ï¼š å¥–åŠ±æ¥è¿‘æœ€ä¼˜è·¯å¾„ã€‚

æ•ˆç‡å­¦ä¹ ï¼š æƒ©ç½šæ¼«æ— ç›®çš„çš„æ¸¸èµ°/å›æº¯ã€‚

è¿›åº¦ä¿¡å·ï¼š æä¾›å¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œè€Œéç¨€ç–çš„æˆåŠŸä¿¡å·ã€‚

å®æ—¶æ€§èƒ½ï¼š è®­ç»ƒé€Ÿåº¦è¶³å¤Ÿå¿«ï¼ˆæ¯æ­¥ 5-10 æ¯«ç§’ï¼‰ã€‚
"""